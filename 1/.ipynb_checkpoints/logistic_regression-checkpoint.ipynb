{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqOjI3thgJ3G"
   },
   "source": [
    "# Logistic Regression\n",
    "+ Simple logistic regression with SGD optimization.\n",
    "+ Predict class fail or pass  with information on number of lectures attendance and hours spent on the final project.\n",
    "+ Data: pass with 4 lectures taken and 10 hours of the final project, but fail with 10 lectures and 3  hours.\n",
    "+ Problem: Will I pass with 6 lectures taken with 4 hours for the final project ?\n",
    "+ It is noted that the derivative of weights are the same as that of linear regression.\n",
    "+ The only difference with the code of linear regression is the sigmoid function for forward processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JqjY8dm3gJ3O"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "eta = 0.7   # learning rate\n",
    "epoch = 10000 # iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OInAx-S2gJ3P"
   },
   "source": [
    "### Logistic Regression Model\n",
    "+ In forward processing, it uses sigmoid activation function. \n",
    "+ The CE (cross-entropy) loss function is used for loss evaluation.\n",
    "+ In backward processing, delta = output - target. It is indentical to that of + linear regression although the loss and activation functions are different.\n",
    "+ Refer to the course note for derivation of delta equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "j6sMjhiPgJ3P"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+ np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "# Logistic Regression Model\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, x, w, y):\n",
    "        self.inputs  = x\n",
    "        self.weights = w               \n",
    "        self.target  = y\n",
    "        self.output  = np.zeros(self.target.shape)\n",
    "\n",
    "    def forward_proc(self):\n",
    "       # forward processing of inputs and weights using sigmoid activation function \n",
    "        self.output = sigmoid(np.dot(self.weights, self.inputs.T))\n",
    "\n",
    "    def backprop(self):\n",
    "        # backward processing of appling the chain rule to find derivative of the mean square error function with respect to weights\n",
    "        dw = (self.output - self.target) * self.inputs # same formular for both linear and logistic regression\n",
    "\n",
    "        # update the weights with the derivative of the loss function\n",
    "        self.weights -= eta * dw\n",
    "\n",
    "    def predict(self, x):\n",
    "        # predict the output for a given input x\n",
    "        return (sigmoid(np.dot(self.weights, x.T)))\n",
    "        \n",
    "    def calculate_error(self):\n",
    "        # calculate error\n",
    "        error = -self.target * math.log(self.output) - (1-self.target) * math.log(1-self.output)\n",
    "        return abs(error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIDxDx_WgJ3P"
   },
   "source": [
    "### SGD (Stochastic Gradient Descent) Optimization\n",
    "+ Train with SGD optimization.\n",
    "+ In SGD, each input data is trained separately with other input data.\n",
    "+ After training, the weights are adjusted to generate the target data for the given input data.\n",
    "+ Check how the loss decreases as the iterations increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m2HhXqPOgJ3Q",
    "outputId": "25fd30cc-84a9-41ef-d232-a1aa11e86f04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weights: [[0.48905873 0.82169489]]\n",
      "Loss:  [0.40783116]\n",
      "Loss:  [0.00346006]\n",
      "Loss:  [0.00165909]\n",
      "Loss:  [0.00114778]\n",
      "Loss:  [0.0008602]\n",
      "Loss:  [0.00066195]\n",
      "Loss:  [0.00055144]\n",
      "Loss:  [0.00047255]\n",
      "Loss:  [0.00042958]\n",
      "Loss:  [0.00038179]\n",
      "Adjusted Weights: [[-11.79597373  12.73287772]]\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # data normalization on number of rooms and age of the house\n",
    "\n",
    "    input_data = np.array(\n",
    "                  [[.4, 1.0, 1.0],\n",
    "                  [1.0, .3, 0.0]])\n",
    "          \n",
    "    \n",
    "    '''\n",
    "    target = [[1.0],  # fail\n",
    "              [0.0]]  # pass\n",
    "              \n",
    "    '''\n",
    "\n",
    "    weights = np.random.rand(1, 2)\n",
    "    print(\"Initial Weights:\", weights)\n",
    "\n",
    "  \n",
    "    # SGD Optimization\n",
    "    for i in range(epoch):\n",
    "   \n",
    "        if i == 0: w = weights\n",
    "\n",
    " \n",
    "        np.random.shuffle(input_data) # shuffle the input data\n",
    "        X = input_data[:, 0:2]\n",
    "        y = input_data[:, 2:3]\n",
    "  \n",
    "        # eta *= 0.95  # decreasing learning rate is found to be not good for this case\n",
    "\n",
    "        for j in range(len(X)): \n",
    "       \n",
    "            model = LogisticRegression(X[j], w, y[j])\n",
    "            model.forward_proc()   # forward processing\n",
    "            model.backprop()       # backward processing\n",
    "            w = model.weights \n",
    "\n",
    "        if (i % 1000) == 0:\n",
    "             print(\"Loss: \", model.calculate_error())\n",
    "        \n",
    "    #print(\"Output:\", model.output)\n",
    "    print(\"Adjusted Weights:\", model.weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQCVi50TgJ3R"
   },
   "source": [
    "### Testing and Prediction\n",
    "+ After training, you can verify that the required target is generated for a given input data.\n",
    "+ During testing phase, new input data is feeded to check the output.\n",
    "+ With new input data, the output is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8eq--cRkgJ3S",
    "outputId": "3032e327-8573-40b7-fbb8-5196977f763c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for the input data [.4, 1.0]: [[0.99966947]]\n",
      "Output for the input data [1.0, 0.3]: [[0.00034346]]\n",
      "Predicted data based on trained weights: \n",
      "Input (scaled):  [[0.6 0.4]]\n",
      "Output probability is :  [[0.12084705]]\n",
      "Predicted output is Fail.\n"
     ]
    }
   ],
   "source": [
    "    # verify the output with the adjusted weights\n",
    "    x1 = np.array([[0.4, 1.0]])\n",
    "    print (\"Output for the input data [.4, 1.0]:\", model.predict(x1))\n",
    "    x2 = np.array([[1.0, 0.3]])\n",
    "    print (\"Output for the input data [1.0, 0.3]:\", model.predict(x2))\n",
    "    \n",
    "    # predicting and testing the output for a given input data\n",
    "    x_prediction = np.array([[0.6, 0.4]])\n",
    "    predicted_output = model.predict(x_prediction)\n",
    "    print(\"Predicted data based on trained weights: \")\n",
    "    print(\"Input (scaled): \", x_prediction)\n",
    "    print(\"Output probability is : \", predicted_output)\n",
    "    if predicted_output >= 0.5:\n",
    "        print(\"Predicted output is PASS.\")\n",
    "    elif predicted_output < 0.5:\n",
    "        print(\"Predicted output is Fail.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SS-feVbgJ3S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XXLCJtZgJ3T"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLExWldMgJ3T"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "logistic_regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
