{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqOjI3thgJ3G"
   },
   "source": [
    "# Logistic Regression\n",
    "+ Simple logistic regression with SGD optimization.\n",
    "+ Predict class fail or pass  with information on number of lectures attendance and hours spent on the final project.\n",
    "+ Data: pass with 4 lectures taken and 10 hours of the final project, but fail with 10 lectures and 3  hours.\n",
    "+ Problem: Will I pass with 6 lectures taken with 4 hours for the final project ?\n",
    "+ It is noted that the derivative of weights are the same as that of linear regression.\n",
    "+ The only difference with the code of linear regression is the sigmoid function for forward processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "JqjY8dm3gJ3O"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "eta = 0.6  # learning rate\n",
    "epoch = 8000 # iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OInAx-S2gJ3P"
   },
   "source": [
    "### Logistic Regression Model\n",
    "+ In forward processing, it uses sigmoid activation function. \n",
    "+ The CE (cross-entropy) loss function is used for loss evaluation.\n",
    "+ In backward processing, delta = output - target. It is indentical to that of + linear regression although the loss and activation functions are different.\n",
    "+ Refer to the course note for derivation of delta equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "j6sMjhiPgJ3P"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0/(1+ np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1.0 - x)\n",
    "\n",
    "# Logistic Regression Model\n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self, x, w, y):\n",
    "        self.inputs  = x\n",
    "        self.weights = w               \n",
    "        self.target  = y\n",
    "        self.output  = np.zeros(self.target.shape)\n",
    "\n",
    "    def forward_proc(self):\n",
    "       # forward processing of inputs and weights using sigmoid activation function \n",
    "        self.output = sigmoid(np.dot(self.weights, self.inputs.T))\n",
    "\n",
    "    def backprop(self):\n",
    "        # backward processing of appling the chain rule to find derivative of the mean square error function with respect to weights\n",
    "        dw = (self.output - self.target) * self.inputs # same formular for both linear and logistic regression\n",
    "\n",
    "        # update the weights with the derivative of the loss function\n",
    "        self.weights -= eta * dw\n",
    "\n",
    "    def predict(self, x):\n",
    "        # predict the output for a given input x\n",
    "        return (sigmoid(np.dot(self.weights, x.T)))\n",
    "        \n",
    "    def calculate_error(self):\n",
    "        # calculate error\n",
    "        error = -self.target * math.log(self.output) - (1-self.target) * math.log(1-self.output)\n",
    "        return abs(error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIDxDx_WgJ3P"
   },
   "source": [
    "### SGD (Stochastic Gradient Descent) Optimization\n",
    "+ Train with SGD optimization.\n",
    "+ In SGD, each input data is trained separately with other input data.\n",
    "+ After training, the weights are adjusted to generate the target data for the given input data.\n",
    "+ Check how the loss decreases as the iterations increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "# load dataset\n",
    "df = pd.read_csv(\"titanic_data.csv\") \n",
    "df.dropna(inplace=True)\n",
    "# preprocess dataset by changing the string to integer, and filling\n",
    "\n",
    "df['Sex'] = df['Sex'].map({'female':1,'male':0})\n",
    "df['Age'].fillna(value=df['Age'].mean(), inplace=True) \n",
    "# initially experiment with 100 samples. For final run, use full\n",
    "# dataset\n",
    "df = df.iloc[:, :]\n",
    "# select proper features for prediction\n",
    "passengers = df[[\"Sex\", \"Age\", \"Pclass\",\"Survived\" ]] \n",
    "# split train and test set\n",
    "train, test = train_test_split(passengers, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m2HhXqPOgJ3Q",
    "outputId": "25fd30cc-84a9-41ef-d232-a1aa11e86f04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weights: [[ 3.49045165 -0.80894269 -0.73375843]]\n",
      "Loss:  [0.04373265]\n",
      "Loss:  [0.04867705]\n",
      "Loss:  [0.05974846]\n",
      "Loss:  [0.81240031]\n",
      "Loss:  [0.45813585]\n",
      "Loss:  [0.03752288]\n",
      "Loss:  [0.48504396]\n",
      "Loss:  [0.48504396]\n",
      "Output: [0.3891805]\n",
      "Adjusted Weights: [[ 3.48977239 -0.80866525 -0.72782058]]\n"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    test_features = test[[\"Sex\", \"Age\", \"Pclass\"]].values\n",
    "    test_labels = test[[\"Survived\"]].values\n",
    "    scaler = MinMaxScaler()\n",
    "    test_features = scaler.fit_transform(test_features) \n",
    "\n",
    "    target = [[1.0], [0.0]]  \n",
    "              \n",
    "    #weights = np.random.rand(1, 3)\n",
    "    weights = np.array([3.49045165, -0.80894269, -0.73375843 ])\n",
    "    weights = weights.reshape(1,3)\n",
    "    print(\"Initial Weights:\", weights)\n",
    "\n",
    "  \n",
    "    # SGD Optimization\n",
    "    for i in range(epoch):\n",
    "   \n",
    "        if i == 0: w = weights\n",
    "        concat_data=np.concatenate((train_features, train_labels), axis = 1)\n",
    "        np.random.shuffle(concat_data) # shuffle the training dataset \n",
    "\n",
    "        X = concat_data[:, 0:3]\n",
    "        y = concat_data[:, 3:4]\n",
    "  \n",
    "        eta *= 0.95  # decreasing learning rate is found to be not good for this case\n",
    "\n",
    "        for j in range(len(X)): \n",
    "       \n",
    "            model = LogisticRegression(X[j], w, y[j])\n",
    "            model.forward_proc()   # forward processing\n",
    "            model.backprop()       # backward processing\n",
    "            w = model.weights \n",
    "\n",
    "        if (i % 1000) == 0:\n",
    "             print(\"Loss: \", model.calculate_error())\n",
    "        \n",
    "    print(\"Output:\", model.output)\n",
    "    print(\"Adjusted Weights:\", model.weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQCVi50TgJ3R"
   },
   "source": [
    "### Testing and Prediction\n",
    "+ After training, you can verify that the required target is generated for a given input data.\n",
    "+ During testing phase, new input data is feeded to check the output.\n",
    "+ With new input data, the output is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "0SS-feVbgJ3S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[13  0]\n",
      " [ 7 17]]\n",
      "Classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      1.00      0.79        13\n",
      "           1       1.00      0.71      0.83        24\n",
      "\n",
      "    accuracy                           0.81        37\n",
      "   macro avg       0.82      0.85      0.81        37\n",
      "weighted avg       0.88      0.81      0.81        37\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "X = test_features\n",
    "y = test_labels\n",
    "w = model.weights # use the weights resulting from training\n",
    "y_predic = []\n",
    "for j in range(len(X)):\n",
    "    model = LogisticRegression(X[j], w, y[j])\n",
    "    if model.predict(X[j]) >= 0.5:\n",
    "        y_predic.append(1)\n",
    "    elif model.predict(X[j]) < 0.5:\n",
    "        y_predic.append(0) \n",
    "results = confusion_matrix(y, y_predic)\n",
    "print ('Confusion Matrix :')\n",
    "print(results)\n",
    "print ('Classification Report : ')\n",
    "print (classification_report(y, y_predic)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "logistic_regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
